{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasirKhan1811/Artificial_Intelligence/blob/main/NLP_with_NLTK_Package.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm3celYO8ZAi"
      },
      "source": [
        "# Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLE0lIis8feG"
      },
      "source": [
        "Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpDjOzHc8zJd"
      },
      "source": [
        "A lot of the data that we could be analyzing is **unstructured data** and contains human-readable text. Before we can analyze that data programmatically, we first need to preprocess it. In this tutorial, we’ll take our first look at the kinds of **text preprocessing** tasks we can do with NLTK so that we’ll be ready to apply them in future projects. We’ll also see how to do some basic text analysis and create visualizations.\n",
        "\n",
        "By the end of this tutorial, we’ll know how to:\n",
        "\n",
        "1. Find text to analyze\n",
        "2. Preprocess our text for analysis\n",
        "3. Analyze our text\n",
        "4. Create visualizations based on our analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BQiuoh43p7"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCuaMUMpvPn1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twfIq2nRG2l4"
      },
      "outputs": [],
      "source": [
        "string = \"\"\"Muad'Dib learned rapidly because his first training was in how to learn.\n",
        "            And the first lesson of all was the basic trust that he could learn.\n",
        "            It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zubs0f3Idux",
        "outputId": "a054daee-00ba-4706-86e7-7eb385d70aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvljnJCH57n",
        "outputId": "c795aded-c55e-4482-dad1-36b884a905b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
              " 'And the first lesson of all was the basic trust that he could learn.',\n",
              " \"It's shocking to find how many people do not believe they can learn, and how many more believe learning to be difficult.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sent_tokenize(string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGGEErs4HYcq",
        "outputId": "08d6fc29-d591-465f-ace5-525bcf5ff63f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Muad'Dib\",\n",
              " 'learned',\n",
              " 'rapidly',\n",
              " 'because',\n",
              " 'his',\n",
              " 'first',\n",
              " 'training',\n",
              " 'was',\n",
              " 'in',\n",
              " 'how',\n",
              " 'to',\n",
              " 'learn',\n",
              " '.',\n",
              " 'And',\n",
              " 'the',\n",
              " 'first',\n",
              " 'lesson',\n",
              " 'of',\n",
              " 'all',\n",
              " 'was',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'trust',\n",
              " 'that',\n",
              " 'he',\n",
              " 'could',\n",
              " 'learn',\n",
              " '.',\n",
              " 'It',\n",
              " \"'s\",\n",
              " 'shocking',\n",
              " 'to',\n",
              " 'find',\n",
              " 'how',\n",
              " 'many',\n",
              " 'people',\n",
              " 'do',\n",
              " 'not',\n",
              " 'believe',\n",
              " 'they',\n",
              " 'can',\n",
              " 'learn',\n",
              " ',',\n",
              " 'and',\n",
              " 'how',\n",
              " 'many',\n",
              " 'more',\n",
              " 'believe',\n",
              " 'learning',\n",
              " 'to',\n",
              " 'be',\n",
              " 'difficult',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "tokens = word_tokenize(string)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ItR96MJXkC"
      },
      "source": [
        "See how \"It's\" was split at the apostrophe to give us 'It' and \"'s\", but \"Muad'Dib\" was left whole? This happened because NLTK knows that 'It' and \"'s\" (a contraction of “is”) are two distinct words, so it counted them separately. But \"Muad'Dib\" isn’t an accepted contraction like \"It's\", so it wasn’t read as two separate words and was left intact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6I84eoL5G9L"
      },
      "source": [
        "### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRK7cohfRk4n"
      },
      "outputs": [],
      "source": [
        "doc = \"Sir, I protest. I am not a merry man!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez3GhWPrUFIJ"
      },
      "source": [
        "Removing punctuations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVAk5D1LTsnX"
      },
      "outputs": [],
      "source": [
        "tokens = [token for token in word_tokenize(doc.lower())\n",
        "          if token.isalpha()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_dZSUTiU3Zw",
        "outputId": "8fc774df-b1a2-48ed-8b70-267c156b5402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sir', 'i', 'protest', 'i', 'am', 'not', 'a', 'merry', 'man']\n"
          ]
        }
      ],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6yyICPMVAbe"
      },
      "source": [
        "Removing stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYKx6Exg58Bq",
        "outputId": "34735b5a-7f25-4026-80e1-f114c02906ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ddnqAa9WXsA"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHh5j7cFU7RL",
        "outputId": "587d5969-660a-4802-fc33-fe521a8ce157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sir', 'protest', 'merry', 'man']\n"
          ]
        }
      ],
      "source": [
        "words = [token for token in tokens if token not in stop_words]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmILHGYScPae"
      },
      "source": [
        "Words like 'I' and 'not' may seem too important to filter out, and depending on what kind of analysis we want to conduct, they can be. Here is why:\n",
        "\n",
        "* 'I' is a pronoun, which is a context word rather than content word:\n",
        "\n",
        "  * **Content words** give us information about the topics covered in the text or the sentiment that the author has about those topics.\n",
        "\n",
        "  * **Context words** give us information about the writing style. We can observe patterns in how authors use context words in order to quantify their writing style. Once we have quantified their writing style, we can analyze a text written by an unknown author to see how closely it follows a particular writing style so we can try to identify who the author is.\n",
        "\n",
        "* 'not' is technically an adverb but has still been included in NLTK’s list of stop words for English. If we want to edit the list of stop words to exclude 'not' or make other changes, then we can download it.\n",
        "\n",
        "So, 'I' and 'not' can be important parts of a sentence/text, but it depends on what we are trying to learn from that sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxVDFOabf13p"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming is a text processing task in which we reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmGRi1uVc2Ri"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRXo9PfXgOae"
      },
      "outputs": [],
      "source": [
        "string_for_stemming = \"\"\"The crew of the USS Discovery discovered many discoveries. Discovering is what explorers do.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QUPtP0vgk4l"
      },
      "source": [
        "Before we can stem the words in the string, we need to separate all the words in it by word_tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEbr4BeFg2gQ",
        "outputId": "121dc26e-9120-440a-8786-4eedc12e234b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'crew',\n",
              " 'of',\n",
              " 'the',\n",
              " 'uss',\n",
              " 'discoveri',\n",
              " 'discov',\n",
              " 'mani',\n",
              " 'discoveri',\n",
              " '.',\n",
              " 'discov',\n",
              " 'is',\n",
              " 'what',\n",
              " 'explor',\n",
              " 'do',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "stemmed_words = [stemmer.stem(word) for word in word_tokenize(string_for_stemming)]\n",
        "stemmed_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtzeDtQ8jYSS"
      },
      "source": [
        "### Tagging Parts of Speech\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzskgneAsUaO",
        "outputId": "7e1dc86c-b276-4b3a-c3c8-f8f573e3a416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVx82pgvqxiQ",
        "outputId": "23741055-e5ef-463d-816f-45444a3f5423"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('If', 'ADP'),\n",
              " ('you', 'PRON'),\n",
              " ('wish', 'VERB'),\n",
              " ('to', 'PRT'),\n",
              " ('make', 'VERB'),\n",
              " ('an', 'DET'),\n",
              " ('apple', 'NOUN'),\n",
              " ('pie', 'NOUN'),\n",
              " ('from', 'ADP'),\n",
              " ('scratch', 'NOUN'),\n",
              " (',', '.'),\n",
              " ('you', 'PRON'),\n",
              " ('must', 'VERB'),\n",
              " ('first', 'VERB'),\n",
              " ('invent', 'VERB'),\n",
              " ('the', 'DET'),\n",
              " ('universe', 'NOUN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# here is a quote of Carl Sagan\n",
        "quote = \"\"\"If you wish to make an apple pie from scratch, you must first invent the universe.\"\"\"\n",
        "\n",
        "# now let's tag the parts of speech in this text\n",
        "nltk.pos_tag(word_tokenize(quote), tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZouH7fo1uWqh"
      },
      "source": [
        "### Lemmatization\n",
        "Lemmatization reduces words to their core/root forms, but it will give us a complete English word that makes sense on its own.\n",
        "\n",
        "> Note: A lemma is a word that represents a whole group of words, and that group of words is called lexeme. For example, if we were to look up the word “blending” in a dictionary, then we would need to look at the entry for “blend” and we would find “blending” listed in that entry. In this example, “blend” is the lemma, and “blending” is part of the lexeme. So when we lemmatize a word, we are reducing it to its lemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GFVmtw1uznr"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNkKtF3ver3"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82f3h5KkvpVy"
      },
      "source": [
        "Let's start with lemmatizing a plural noun:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-i3hmx_v-IC",
        "outputId": "9904f864-ac81-4529-cd66-d768e7ac35ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# first install the 'wordnet' corpus\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4jRiYRQNvi15",
        "outputId": "ff984fd6-e056-4689-a00c-34c36efb9c1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'keep'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"keeps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P82BCIZvwqnA",
        "outputId": "502edf42-3b38-47a8-998a-e3eb99f73b85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "[lemmatizer.lemmatize(token) for token in word_tokenize(\"The friends of DeSoto love scarves.\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SpxzVkyxVsa"
      },
      "source": [
        "That looks right. The plurals 'friends' and 'scarves' became the singulars 'friend' and 'scarf'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8JIsxiTxXL1"
      },
      "source": [
        "But what would happen if you lemmatized a word that looked very different from its lemma? Try lemmatizing \"worst\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CTN3vcUHwzAM",
        "outputId": "95a89f67-a8c2-4013-857e-18e02d27a52d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'worst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"worst\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcZxwx-GxqCK"
      },
      "source": [
        "Here we got the result 'worst' because lemmatizer.lemmatize() assumed that \"worst\" was a noun. We can make it clear that we want \"worst\" to be an adjective:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vthNabnPxwx2",
        "outputId": "69dabf35-2c77-4ecb-9a4e-cc37612ca01c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bad'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew_AnQ97yPO_"
      },
      "source": [
        "The default parameter for pos is 'n' for noun, but we made sure that \"worst\" was treated as an adjective by adding the parameter **pos=\"a\"**. As a result, we got 'bad', which looks very different from the original word and is nothing like what we would get in stemming. This is because \"worst\" is the superlative form of the adjective 'bad', and lemmatizing reduces superlatives as well as comparatives to their lemmas.\n",
        "\n",
        "Now that we know how to use NLTK to tag parts of speech, we can try tagging words before lemmatizing them to avoid mixing up **homographs**, or words that are spelled the same but have different meanings and can be different parts of speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2REnI4qFHRT"
      },
      "source": [
        "### Chunking and Chinking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y65LoVpVFKQY"
      },
      "source": [
        "\"Chunking involves grouping words into meaningful chunks based on their part-of-speech tags.\"\n",
        "\n",
        "> Purpose: It helps in identifying and extracting phrases like noun phrases, verb phrases, prepositional phrases, etc., which provide more context and meaning to the text.\n",
        ">\n",
        "> Technique: Chunking is typically performed using regular expressions or rule-based approaches to define patterns for identifying and extracting chunks.\n",
        ">\n",
        "> Example: Identifying noun phrases like \"the quick brown fox\" or verb phrases like \"jumps over\" in a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cymVJylqGXGT"
      },
      "source": [
        "Before we chunk, we need to make sure that the parts of speech in our text are tagged. So we create a string for POS tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnJ8AcJDLHar"
      },
      "source": [
        "\"Chinking is the process of excluding certain parts from a chunk that match specific patterns while chunking.\"\n",
        "\n",
        "> Purpose: It allows for refining the chunking process by excluding certain words or patterns that should not be included in a chunk.\n",
        ">\n",
        "> Technique: Chinking involves specifying patterns that should be excluded from a chunk while defining the chunking rules.\n",
        ">\n",
        "> Example: Excluding determiners or adjectives from a noun phrase chunk to create more precise chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJmQDfxqLnGG"
      },
      "source": [
        "**Example:**\n",
        "\n",
        "Consider the sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
        "> Chunking Output:\n",
        "* Noun Phrases: \"The quick brown fox\", \"the lazy dog\"\n",
        "* Verb Phrases: \"jumps over\"\n",
        "\n",
        "> Chinking Output:\n",
        "* Noun Phrases: \"quick brown fox\"\n",
        "* Verb Phrase: \"jumps\"\n",
        "\n",
        "By combining chunking and chinking techniques, NLP systems can extract structured information from text data, enabling more advanced analysis such as information extraction, named entity recognition, and relationship extraction. These techniques play a crucial role in syntactic analysis and text processing tasks in NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM66hhDdLUHd",
        "outputId": "50dbe3e6-3bc7-401f-c825-144c0ef5a5f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PRP'),\n",
              " (\"'s\", 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('dangerous', 'JJ'),\n",
              " ('business', 'NN'),\n",
              " (',', ','),\n",
              " ('Frodo', 'NNP'),\n",
              " (',', ','),\n",
              " ('going', 'VBG'),\n",
              " ('out', 'RP'),\n",
              " ('your', 'PRP$'),\n",
              " ('door', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\"\n",
        "tags = nltk.pos_tag(word_tokenize(lotr_quote))\n",
        "tags"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition (NER)"
      ],
      "metadata": {
        "id": "zlsi3QRZh9ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entities are noun phrases that refer to specific locations, people, organizations, and so on. With named entity recognition, we can find the named entities in our texts and also determine what kind of named entity they are."
      ],
      "metadata": {
        "id": "Xl8aTJ-OvSyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF3XNWMdjdWU",
        "outputId": "7f6bc856-9b5d-4c0f-addf-1341e809646d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52rODEGqr1-",
        "outputId": "ef8c9b12-da38-4445-f8a9-6c6cfc44aea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8VaQqhirz1F",
        "outputId": "6e26c1f1-e4c9-4689-95a4-921a9900e664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 01:**"
      ],
      "metadata": {
        "id": "9i9x9dU-zvNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example text\n",
        "text = 'In New York, I like to ride the Metro to visit MOMA'\n",
        "\n",
        "# tokenizing and pos_tagging\n",
        "tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "\n",
        "# NER\n",
        "named_entities = nltk.ne_chunk(tagged)\n",
        "\n",
        "# instantiating an empty dictionary\n",
        "from collections import defaultdict\n",
        "ne_categories = defaultdict(int)\n",
        "\n",
        "# count chunk types\n",
        "for chunk in named_entities:\n",
        "  if hasattr(chunk, 'label'):\n",
        "    ne_categories[chunk.label()] += 1\n",
        "\n",
        "print(ne_categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AX_JUmhq_eR",
        "outputId": "8c12c688-421d-4575-9afe-10a396794238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'GPE': 1, 'ORGANIZATION': 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 02:**"
      ],
      "metadata": {
        "id": "DnereH7yzqDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quote = \"\"\"Men like Schiaparelli watched the red planet—it is odd, by-the-bye, that for countless centuries Mars has been the star of war—but failed to interpret the fluctuating appearances of the markings they mapped so well. All that time the Martians must have been getting ready. During the opposition of 1894 a great light was seen on the illuminated part of the disk, first at the Lick Observatory, then by Perrotin of Nice, and then by other observers. English readers heard of it first in the issue of Nature dated August 2.\"\"\""
      ],
      "metadata": {
        "id": "30H-oQjqwJJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ne(quote):\n",
        "  words = word_tokenize(quote)\n",
        "  tags = nltk.pos_tag(words)\n",
        "  tree = nltk.ne_chunk(tags, binary=True)\n",
        "  return set(\" \".join(entity[0] for entity in ne) for ne in tree if hasattr(ne, \"label\") and ne.label() == \"NE\")"
      ],
      "metadata": {
        "id": "pzkz_BVKwTJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_ne(quote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC5VEgGaxjsa",
        "outputId": "614e5e0d-8abc-4c31-ff1c-569491832d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Lick Observatory', 'Mars', 'Nature', 'Perrotin', 'Schiaparelli'}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 03:**"
      ],
      "metadata": {
        "id": "3ecjqlmizEgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text to be analyzed\n",
        "text = \"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Apply part-of-speech tagging to the tokens\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "# Apply named entity recognition to the tagged words\n",
        "ne_tree = nltk.ne_chunk(tagged, binary=True)\n",
        "\n",
        "# Print the named entities\n",
        "for chunk in ne_tree:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        print(chunk.label(), '-->', ' '.join(c for c in chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa7U1llAzIC9",
        "outputId": "a3ac893c-d3b7-42cb-e243-c1e4f2988ff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NE --> European\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "o4BQiuoh43p7",
        "Q6I84eoL5G9L",
        "dxVDFOabf13p",
        "GtzeDtQ8jYSS"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyM03SZXZmDILyn6dsjhbev9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}